{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad156ab",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "Run the cell below to install Python packages (non-PyTorch).\n",
    "For PyTorch + CUDA, please follow the official installation command that matches your CUDA version (examples provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d54e9",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Install general Python packages used by the notebook.\n",
    "# NOTE: PyTorch installation depends on your CUDA version and is not installed by this single pip command.\n",
    "# If you use conda and want GPU-enabled PyTorch (recommended), run one of the example commands below in a terminal.\n",
    "\n",
    "# Example conda (CUDA 11.8) - run in a terminal:\n",
    "# conda install -y pytorch torchvision torchaudio cudatoolkit=11.8 -c pytorch\n",
    "\n",
    "# Example pip (CUDA 11.8) - run in a terminal or here if appropriate:\n",
    "# pip install --index-url https://download.pytorch.org/whl/cu118 torch torchvision torchaudio --upgrade\n",
    "\n",
    "# Install the rest of the requirements (this installs matplotlib, tqdm, pillow, etc.).\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "print('Requirements installation attempted.\n",
    "Please ensure PyTorch with CUDA is installed separately if you need GPU support.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce41312",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ee926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torchvision\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.retinanet import RetinaNetHead\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d51e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick GPU test: small tensor operations on GPU (if available)\n",
    "import torch\n",
    "print('torch version:', torch.__version__)\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.device('cuda')\n",
    "    x = torch.randn((1024, 1024), device=dev)\n",
    "    y = x * 2.0\n",
    "    z = (x + y).sum()\n",
    "    print('Tensor op result on', x.device, '->', z.item())\n",
    "else:\n",
    "    print('CUDA not available in this kernel. If you expect GPU support, install PyTorch with CUDA and restart the kernel.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349825f",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bdbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_PATH = r'D:\\CLID\\IDD-CPLID.v3-cplid_new.coco'\n",
    "    OUTPUT_DIR = 'checkpoints'\n",
    "    \n",
    "    # Model\n",
    "    NUM_CLASSES = 3  # background (0 is used internally), defect (1), insulator (2)\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 30\n",
    "    NUM_WORKERS = 4\n",
    "    \n",
    "    # Optimizer\n",
    "    LEARNING_RATE = 0.001\n",
    "    MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "    \n",
    "    # Learning Rate Scheduler\n",
    "    LR_SCHEDULER = 'cosine'  # 'step', 'cosine', 'onecycle'\n",
    "    WARMUP_EPOCHS = 3\n",
    "    \n",
    "    # Mixed Precision\n",
    "    USE_AMP = True\n",
    "    \n",
    "    # Gradient Clipping\n",
    "    GRAD_CLIP = 1.0\n",
    "    \n",
    "    # Early Stopping\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    \n",
    "    # Data Augmentation\n",
    "    USE_AUGMENTATION = True\n",
    "    \n",
    "    # Evaluation\n",
    "    SCORE_THRESHOLD = 0.5\n",
    "    IOU_THRESHOLD = 0.5\n",
    "    \n",
    "    # Class names\n",
    "    CLASS_NAMES = {0: 'background', 1: 'defect', 2: 'insulator'}\n",
    "    CLASS_COLORS = {1: 'red', 2: 'green'}\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36c9bd",
   "metadata": {},
   "source": [
    "## 3. Dataset Definition with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation_file, transforms=None, augment=False):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        # Map image_id to filename and other info\n",
    "        self.images = {img['id']: img for img in self.coco_data['images']}\n",
    "        \n",
    "        # Map image_id to list of annotations\n",
    "        self.img_to_anns = {img['id']: [] for img in self.coco_data['images']}\n",
    "        for ann in self.coco_data['annotations']:\n",
    "            if ann['image_id'] in self.img_to_anns:\n",
    "                self.img_to_anns[ann['image_id']].append(ann)\n",
    "        \n",
    "        # List of image IDs for indexing\n",
    "        self.ids = list(self.images.keys())\n",
    "        \n",
    "        # Categories mapping\n",
    "        self.categories = {cat['id']: cat['name'] for cat in self.coco_data['categories']}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        img_info = self.images[img_id]\n",
    "        file_name = img_info['file_name']\n",
    "        \n",
    "        # Load Image\n",
    "        img_path = os.path.join(self.root, file_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get Annotations\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            # COCO bbox: [x, y, w, h] -> PyTorch: [x1, y1, x2, y2]\n",
    "            x, y, w, h = ann['bbox']\n",
    "            # Skip invalid boxes\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "            areas.append(ann['area'])\n",
    "            iscrowd.append(ann.get('iscrowd', 0))\n",
    "        \n",
    "        if len(boxes) > 0:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "            iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        else:\n",
    "            # Negative example (no objects)\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            areas = torch.zeros((0,), dtype=torch.float32)\n",
    "            iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([img_id])\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": areas,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "            img, target = self.apply_augmentations(img, target)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def apply_augmentations(self, img, target):\n",
    "        \"\"\"Apply data augmentations\"\"\"\n",
    "        boxes = target['boxes']\n",
    "        \n",
    "        # Random Horizontal Flip\n",
    "        if random.random() > 0.5:\n",
    "            img = F.hflip(img)\n",
    "            if len(boxes) > 0:\n",
    "                w = img.width\n",
    "                boxes[:, [0, 2]] = w - boxes[:, [2, 0]]\n",
    "        \n",
    "        # Color Jittering\n",
    "        if random.random() > 0.5:\n",
    "            brightness = random.uniform(0.8, 1.2)\n",
    "            contrast = random.uniform(0.8, 1.2)\n",
    "            saturation = random.uniform(0.8, 1.2)\n",
    "            img = F.adjust_brightness(img, brightness)\n",
    "            img = F.adjust_contrast(img, contrast)\n",
    "            img = F.adjust_saturation(img, saturation)\n",
    "        \n",
    "        target['boxes'] = boxes\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def get_transform(train=False):\n",
    "    transforms = [ToTensor()]\n",
    "    # Note: RetinaNet handles normalization internally with pretrained weights\n",
    "    return Compose(transforms)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b104cf",
   "metadata": {},
   "source": [
    "## 4. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78384982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "train_dir = os.path.join(config.DATA_PATH, 'train')\n",
    "train_ann = os.path.join(train_dir, '_annotations.coco.json')\n",
    "val_dir = os.path.join(config.DATA_PATH, 'valid')\n",
    "val_ann = os.path.join(val_dir, '_annotations.coco.json')\n",
    "test_dir = os.path.join(config.DATA_PATH, 'test')\n",
    "test_ann = os.path.join(test_dir, '_annotations.coco.json')\n",
    "\n",
    "# Create datasets\n",
    "dataset_train = CustomCocoDataset(\n",
    "    train_dir, train_ann, \n",
    "    transforms=get_transform(train=True), \n",
    "    augment=config.USE_AUGMENTATION\n",
    ")\n",
    "dataset_val = CustomCocoDataset(\n",
    "    val_dir, val_ann, \n",
    "    transforms=get_transform(train=False), \n",
    "    augment=False\n",
    ")\n",
    "dataset_test = CustomCocoDataset(\n",
    "    test_dir, test_ann, \n",
    "    transforms=get_transform(train=False), \n",
    "    augment=False\n",
    ")\n",
    "\n",
    "print(f\"Training images: {len(dataset_train)}\")\n",
    "print(f\"Validation images: {len(dataset_val)}\")\n",
    "print(f\"Test images: {len(dataset_test)}\")\n",
    "\n",
    "# Create data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cb948",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, idx=0, class_names=None, class_colors=None):\n",
    "    \"\"\"Visualize a sample from the dataset with bounding boxes\"\"\"\n",
    "    img, target = dataset[idx]\n",
    "    \n",
    "    # Convert tensor to numpy for visualization\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    boxes = target['boxes']\n",
    "    labels = target['labels']\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        x1, y1, x2, y2 = box.numpy() if isinstance(box, torch.Tensor) else box\n",
    "        label_id = label.item() if isinstance(label, torch.Tensor) else label\n",
    "        \n",
    "        color = class_colors.get(label_id, 'blue') if class_colors else 'red'\n",
    "        label_name = class_names.get(label_id, str(label_id)) if class_names else str(label_id)\n",
    "        \n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2 - x1, y2 - y1,\n",
    "            linewidth=2, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1 - 5, label_name, color=color, fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.title(f'Sample {idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few samples\n",
    "for i in [0, 10, 50]:\n",
    "    visualize_sample(dataset_train, i, config.CLASS_NAMES, config.CLASS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1950df",
   "metadata": {},
   "source": [
    "## 6. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes, pretrained=True):\n",
    "    \"\"\"\n",
    "    Load RetinaNet with ResNet50 FPN backbone.\n",
    "    Replace the classification head for custom number of classes.\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        weights = RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "        model = retinanet_resnet50_fpn_v2(weights=weights)\n",
    "    else:\n",
    "        model = retinanet_resnet50_fpn_v2(weights=None)\n",
    "    \n",
    "    # Get model parameters\n",
    "    in_channels = model.backbone.out_channels\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "    \n",
    "    # Replace head with new one for our number of classes\n",
    "    model.head = RetinaNetHead(\n",
    "        in_channels,\n",
    "        num_anchors,\n",
    "        num_classes,\n",
    "        norm_layer=torch.nn.BatchNorm2d\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = get_model(config.NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da7855",
   "metadata": {},
   "source": [
    "## 7. Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params, \n",
    "    lr=config.LEARNING_RATE, \n",
    "    momentum=config.MOMENTUM, \n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if config.LR_SCHEDULER == 'step':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "elif config.LR_SCHEDULER == 'cosine':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config.NUM_EPOCHS, eta_min=1e-6\n",
    "    )\n",
    "elif config.LR_SCHEDULER == 'onecycle':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config.LEARNING_RATE * 10,\n",
    "        epochs=config.NUM_EPOCHS,\n",
    "        steps_per_epoch=len(data_loader_train)\n",
    "    )\n",
    "else:\n",
    "    lr_scheduler = None\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config.USE_AMP else None\n",
    "\n",
    "print(f\"Optimizer: SGD (lr={config.LEARNING_RATE}, momentum={config.MOMENTUM})\")\n",
    "print(f\"LR Scheduler: {config.LR_SCHEDULER}\")\n",
    "print(f\"Mixed Precision: {config.USE_AMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b537ec",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb939298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, scaler=None, grad_clip=None):\n",
    "    \"\"\"Train for one epoch with mixed precision support\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            scaler.scale(losses).backward()\n",
    "            \n",
    "            if grad_clip is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            losses.backward()\n",
    "            \n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        loss_classifier += loss_dict.get('classification', torch.tensor(0)).item()\n",
    "        loss_box_reg += loss_dict.get('bbox_regression', torch.tensor(0)).item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{losses.item():.4f}',\n",
    "            'cls': f'{loss_dict.get(\"classification\", 0):.4f}',\n",
    "            'box': f'{loss_dict.get(\"bbox_regression\", 0):.4f}'\n",
    "        })\n",
    "    \n",
    "    n = len(data_loader)\n",
    "    return {\n",
    "        'total_loss': total_loss / n,\n",
    "        'cls_loss': loss_classifier / n,\n",
    "        'box_loss': loss_box_reg / n\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1cc5c",
   "metadata": {},
   "source": [
    "## 9. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbffd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute IoU between two boxes\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, score_threshold=0.5, iou_threshold=0.5):\n",
    "    \"\"\"Evaluate model and compute metrics per class\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Metrics per class\n",
    "    class_tp = defaultdict(int)\n",
    "    class_fp = defaultdict(int)\n",
    "    class_fn = defaultdict(int)\n",
    "    class_iou_sum = defaultdict(float)\n",
    "    class_iou_count = defaultdict(int)\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc='Evaluating')\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for output, target in zip(outputs, targets):\n",
    "            gt_boxes = target['boxes'].to(device)\n",
    "            gt_labels = target['labels'].to(device)\n",
    "            \n",
    "            pred_boxes = output['boxes']\n",
    "            pred_labels = output['labels']\n",
    "            pred_scores = output['scores']\n",
    "            \n",
    "            # Filter by score\n",
    "            keep = pred_scores > score_threshold\n",
    "            pred_boxes = pred_boxes[keep]\n",
    "            pred_labels = pred_labels[keep]\n",
    "            pred_scores = pred_scores[keep]\n",
    "            \n",
    "            # Track matched ground truth\n",
    "            gt_matched = [False] * len(gt_boxes)\n",
    "            \n",
    "            # For each prediction\n",
    "            for pb, pl in zip(pred_boxes, pred_labels):\n",
    "                best_iou = 0\n",
    "                best_idx = -1\n",
    "                \n",
    "                # Find best matching GT\n",
    "                for i, (gb, gl) in enumerate(zip(gt_boxes, gt_labels)):\n",
    "                    if gt_matched[i]:\n",
    "                        continue\n",
    "                    if pl.item() != gl.item():\n",
    "                        continue\n",
    "                    \n",
    "                    iou = compute_iou(pb.cpu().numpy(), gb.cpu().numpy())\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_idx = i\n",
    "                \n",
    "                label = pl.item()\n",
    "                if best_iou >= iou_threshold:\n",
    "                    class_tp[label] += 1\n",
    "                    class_iou_sum[label] += best_iou\n",
    "                    class_iou_count[label] += 1\n",
    "                    gt_matched[best_idx] = True\n",
    "                else:\n",
    "                    class_fp[label] += 1\n",
    "            \n",
    "            # Count false negatives (unmatched GT)\n",
    "            for i, (gb, gl) in enumerate(zip(gt_boxes, gt_labels)):\n",
    "                if not gt_matched[i]:\n",
    "                    class_fn[gl.item()] += 1\n",
    "    \n",
    "    # Compute metrics\n",
    "    results = {}\n",
    "    all_tp, all_fp, all_fn = 0, 0, 0\n",
    "    \n",
    "    for label in set(list(class_tp.keys()) + list(class_fn.keys())):\n",
    "        tp = class_tp[label]\n",
    "        fp = class_fp[label]\n",
    "        fn = class_fn[label]\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        avg_iou = class_iou_sum[label] / class_iou_count[label] if class_iou_count[label] > 0 else 0\n",
    "        \n",
    "        results[label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'avg_iou': avg_iou,\n",
    "            'tp': tp, 'fp': fp, 'fn': fn\n",
    "        }\n",
    "        \n",
    "        all_tp += tp\n",
    "        all_fp += fp\n",
    "        all_fn += fn\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_precision = all_tp / (all_tp + all_fp) if (all_tp + all_fp) > 0 else 0\n",
    "    overall_recall = all_tp / (all_tp + all_fn) if (all_tp + all_fn) > 0 else 0\n",
    "    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "    \n",
    "    results['overall'] = {\n",
    "        'precision': overall_precision,\n",
    "        'recall': overall_recall,\n",
    "        'f1': overall_f1,\n",
    "        'tp': all_tp, 'fp': all_fp, 'fn': all_fn\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_metrics(results, class_names):\n",
    "    \"\"\"Pretty print evaluation metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"{'Class':<15} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Avg IoU':>10}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for label, metrics in results.items():\n",
    "        if label == 'overall':\n",
    "            continue\n",
    "        name = class_names.get(label, str(label))\n",
    "        print(f\"{name:<15} {metrics['precision']:>10.4f} {metrics['recall']:>10.4f} \"\n",
    "              f\"{metrics['f1']:>10.4f} {metrics.get('avg_iou', 0):>10.4f}\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    overall = results['overall']\n",
    "    print(f\"{'Overall':<15} {overall['precision']:>10.4f} {overall['recall']:>10.4f} \"\n",
    "          f\"{overall['f1']:>10.4f}\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d6447",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_cls_loss': [],\n",
    "    'train_box_loss': [],\n",
    "    'val_f1': [],\n",
    "    'val_precision': [],\n",
    "    'val_recall': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_f1 = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training for {config.NUM_EPOCHS} epochs...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ba3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_metrics = train_one_epoch(\n",
    "        model, optimizer, data_loader_train, device, epoch,\n",
    "        scaler=scaler, grad_clip=config.GRAD_CLIP\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if lr_scheduler is not None and config.LR_SCHEDULER != 'onecycle':\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    val_results = evaluate(\n",
    "        model, data_loader_val, device,\n",
    "        score_threshold=config.SCORE_THRESHOLD,\n",
    "        iou_threshold=config.IOU_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_metrics['total_loss'])\n",
    "    history['train_cls_loss'].append(train_metrics['cls_loss'])\n",
    "    history['train_box_loss'].append(train_metrics['box_loss'])\n",
    "    history['val_f1'].append(val_results['overall']['f1'])\n",
    "    history['val_precision'].append(val_results['overall']['precision'])\n",
    "    history['val_recall'].append(val_results['overall']['recall'])\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch}/{config.NUM_EPOCHS-1}\")\n",
    "    print(f\"  Train Loss: {train_metrics['total_loss']:.4f} \"\n",
    "          f\"(cls: {train_metrics['cls_loss']:.4f}, box: {train_metrics['box_loss']:.4f})\")\n",
    "    print(f\"  Val F1: {val_results['overall']['f1']:.4f} \"\n",
    "          f\"(P: {val_results['overall']['precision']:.4f}, R: {val_results['overall']['recall']:.4f})\")\n",
    "    print(f\"  LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Print per-class metrics\n",
    "    print_metrics(val_results, config.CLASS_NAMES)\n",
    "    \n",
    "    # Save best model\n",
    "    val_f1 = val_results['overall']['f1']\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_f1': best_f1,\n",
    "            'config': vars(config)\n",
    "        }, os.path.join(config.OUTPUT_DIR, 'best_retinanet.pth'))\n",
    "        print(f\"  âœ“ Saved new best model (F1: {best_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Save last model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_f1': best_f1,\n",
    "        'config': vars(config)\n",
    "    }, os.path.join(config.OUTPUT_DIR, 'last_retinanet.pth'))\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining complete! Best F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec5b38",
   "metadata": {},
   "source": [
    "## 11. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training Loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['train_loss'], label='Total Loss', linewidth=2)\n",
    "ax1.plot(history['train_cls_loss'], label='Classification Loss', linewidth=2)\n",
    "ax1.plot(history['train_box_loss'], label='Box Regression Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Metrics\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['val_f1'], label='F1 Score', linewidth=2)\n",
    "ax2.plot(history['val_precision'], label='Precision', linewidth=2)\n",
    "ax2.plot(history['val_recall'], label='Recall', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Validation Metrics')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['lr'], linewidth=2, color='green')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Learning Rate')\n",
    "ax3.set_title('Learning Rate Schedule')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score (main metric)\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history['val_f1'], linewidth=2, color='blue')\n",
    "ax4.axhline(y=best_f1, color='r', linestyle='--', label=f'Best F1: {best_f1:.4f}')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('F1 Score')\n",
    "ax4.set_title('Validation F1 Score')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.OUTPUT_DIR, 'training_curves.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee32e7",
   "metadata": {},
   "source": [
    "## 12. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b61e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(config.OUTPUT_DIR, 'best_retinanet.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']} (F1: {checkpoint['best_f1']:.4f})\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on Test Set...\")\n",
    "test_results = evaluate(\n",
    "    model, data_loader_test, device,\n",
    "    score_threshold=config.SCORE_THRESHOLD,\n",
    "    iou_threshold=config.IOU_THRESHOLD\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print_metrics(test_results, config.CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864273b",
   "metadata": {},
   "source": [
    "## 13. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_predictions(model, dataset, device, indices=None, num_samples=5, \n",
    "                          score_threshold=0.5, class_names=None, class_colors=None):\n",
    "    \"\"\"Visualize model predictions on sample images\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if indices is None:\n",
    "        indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    \n",
    "    for idx in indices:\n",
    "        img, target = dataset[idx]\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model([img.to(device)])[0]\n",
    "        \n",
    "        # Convert image for display\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Ground Truth\n",
    "        axes[0].imshow(img_np)\n",
    "        axes[0].set_title('Ground Truth')\n",
    "        for box, label in zip(target['boxes'], target['labels']):\n",
    "            x1, y1, x2, y2 = box.numpy()\n",
    "            label_id = label.item()\n",
    "            color = class_colors.get(label_id, 'blue') if class_colors else 'red'\n",
    "            name = class_names.get(label_id, str(label_id)) if class_names else str(label_id)\n",
    "            \n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                     linewidth=2, edgecolor=color, facecolor='none')\n",
    "            axes[0].add_patch(rect)\n",
    "            axes[0].text(x1, y1-5, name, color=color, fontsize=10, fontweight='bold')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Predictions\n",
    "        axes[1].imshow(img_np)\n",
    "        axes[1].set_title('Predictions')\n",
    "        \n",
    "        keep = prediction['scores'] > score_threshold\n",
    "        pred_boxes = prediction['boxes'][keep].cpu()\n",
    "        pred_labels = prediction['labels'][keep].cpu()\n",
    "        pred_scores = prediction['scores'][keep].cpu()\n",
    "        \n",
    "        for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "            x1, y1, x2, y2 = box.numpy()\n",
    "            label_id = label.item()\n",
    "            color = class_colors.get(label_id, 'blue') if class_colors else 'red'\n",
    "            name = class_names.get(label_id, str(label_id)) if class_names else str(label_id)\n",
    "            \n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                     linewidth=2, edgecolor=color, facecolor='none')\n",
    "            axes[1].add_patch(rect)\n",
    "            axes[1].text(x1, y1-5, f'{name}: {score:.2f}', color=color, fontsize=10, fontweight='bold')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualize predictions on test set\n",
    "visualize_predictions(\n",
    "    model, dataset_test, device, \n",
    "    num_samples=6,\n",
    "    score_threshold=config.SCORE_THRESHOLD,\n",
    "    class_names=config.CLASS_NAMES,\n",
    "    class_colors=config.CLASS_COLORS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ef6c4",
   "metadata": {},
   "source": [
    "## 14. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, image_path, device, score_threshold=0.5, class_names=None):\n",
    "    \"\"\"\n",
    "    Run inference on a single image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        image_path: Path to image\n",
    "        device: Device to run on\n",
    "        score_threshold: Confidence threshold\n",
    "        class_names: Dict mapping class IDs to names\n",
    "    \n",
    "    Returns:\n",
    "        List of detections: [{'box': [x1,y1,x2,y2], 'label': str, 'score': float}, ...]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = F.to_tensor(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    outputs = model(img_tensor)[0]\n",
    "    \n",
    "    # Filter by score\n",
    "    keep = outputs['scores'] > score_threshold\n",
    "    boxes = outputs['boxes'][keep].cpu().numpy()\n",
    "    labels = outputs['labels'][keep].cpu().numpy()\n",
    "    scores = outputs['scores'][keep].cpu().numpy()\n",
    "    \n",
    "    # Format results\n",
    "    detections = []\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        detections.append({\n",
    "            'box': box.tolist(),\n",
    "            'label': class_names.get(label, str(label)) if class_names else str(label),\n",
    "            'label_id': int(label),\n",
    "            'score': float(score)\n",
    "        })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# Example usage\n",
    "sample_image = os.path.join(test_dir, os.listdir(test_dir)[0])\n",
    "if sample_image.endswith('.jpg') or sample_image.endswith('.png'):\n",
    "    detections = predict(model, sample_image, device, \n",
    "                        score_threshold=config.SCORE_THRESHOLD, \n",
    "                        class_names=config.CLASS_NAMES)\n",
    "    print(f\"Detections for {os.path.basename(sample_image)}:\")\n",
    "    for det in detections:\n",
    "        print(f\"  {det['label']}: {det['score']:.3f} at {det['box']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf2d08",
   "metadata": {},
   "source": [
    "## 15. Export Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944972f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for inference\n",
    "model.eval()\n",
    "\n",
    "# Save just the model weights (smaller file)\n",
    "torch.save(model.state_dict(), os.path.join(config.OUTPUT_DIR, 'retinanet_weights.pth'))\n",
    "\n",
    "# Save complete model (easier to load)\n",
    "torch.save(model, os.path.join(config.OUTPUT_DIR, 'retinanet_complete.pth'))\n",
    "\n",
    "print(f\"Models saved to {config.OUTPUT_DIR}\")\n",
    "print(f\"  - retinanet_weights.pth (weights only)\")\n",
    "print(f\"  - retinanet_complete.pth (complete model)\")\n",
    "print(f\"  - best_retinanet.pth (checkpoint with optimizer state)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c79fe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Optimizations Made:\n",
    "1. **Mixed Precision Training (AMP)** - Faster training with lower memory usage\n",
    "2. **Data Augmentation** - Random horizontal flip and color jittering\n",
    "3. **Cosine Annealing LR** - Smoother learning rate decay\n",
    "4. **Gradient Clipping** - Training stability\n",
    "5. **Early Stopping** - Prevent overfitting\n",
    "6. **Per-class Metrics** - Better evaluation\n",
    "7. **Visualization Tools** - Sample predictions\n",
    "8. **Clean Inference API** - Easy to use for deployment\n",
    "\n",
    "### Files Saved:\n",
    "- `checkpoints/best_retinanet.pth` - Best model checkpoint\n",
    "- `checkpoints/last_retinanet.pth` - Latest checkpoint\n",
    "- `checkpoints/training_curves.png` - Training visualization\n",
    "- `checkpoints/retinanet_weights.pth` - Model weights only\n",
    "- `checkpoints/retinanet_complete.pth` - Complete model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
